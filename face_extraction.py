import os
import glob
import cv2
import numpy as np
from ultralytics import YOLO
import time
from tabulate import tabulate


def print_status(message, level=0, emoji=""):
    """
    ×”×“×¤×¡×ª ×”×•×“×¢×•×ª ×¡×˜×˜×•×¡ ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•×××•×¨×’× ×ª

    Args:
        message (str): ×”×”×•×“×¢×” ×œ×”×“×¤×¡×”
        level (int): ×¨××ª ×”×”×™×¨×¨×›×™×” (×›××” ×¨×•×•×—×™× ×œ×”×•×¡×™×£)
        emoji (str): ××™××•×’'×™ ××•×¤×¦×™×•× ×œ×™ ×œ×”×•×¡×¤×” ×‘×ª×—×™×œ×ª ×”×”×•×“×¢×”
    """
    indent = "    " * level
    if emoji:
        print(f"{indent}{emoji} {message}")
    else:
        print(f"{indent}{message}")


class FaceExtractor:
    def __init__(self, model_path='./face_yolov8n.pt', output_dir="./EnviroFaces"):
        """
        ××ª×—×•×œ ××—×œ×¥ ×”×¤× ×™×

        Args:
            model_path (str): ×”× ×ª×™×‘ ×œ××•×“×œ YOLO
            output_dir (str): ×ª×™×§×™×™×ª ×”×¤×œ×˜ ×œ×©××™×¨×ª ×”×¤× ×™× ×©×–×•×”×•
        """
        print_status("×××ª×—×œ ××¢×¨×›×ª ×—×™×œ×•×¥ ×¤× ×™×...", emoji="ğŸš€")
        self.yolo_model = YOLO(model_path)
        self.output_dir = output_dir

        # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×”×¤×œ×˜ ×× ×œ× ×§×™×™××ª
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print_status(f"× ×•×¦×¨×” ×ª×™×§×™×™×ª ×¤×œ×˜ ×—×“×©×”: {self.output_dir}", level=1, emoji="ğŸ“")

        # ×¢×¨×›×™ ×¡×£ ×œ×‘×“×™×§×•×ª ××™×›×•×ª
        self.FACE_SIZE_THRESHOLD = 0.000
        self.MIN_SHARPNESS = 000
        self.MAX_NOISE_THRESHOLD = 100
        self.MIN_CONTRAST = 0.0

    def check_face_size(self, face_area, image_area):
        """×‘×“×™×§×ª ×’×•×“×œ ×”×¤× ×™× ×‘×™×—×¡ ×œ×ª××•× ×”"""
        face_ratio = face_area / image_area

        is_big_enough = face_ratio >= self.FACE_SIZE_THRESHOLD
        if is_big_enough:
            return is_big_enough, face_ratio
        else:
            return is_big_enough, face_ratio

    def check_face_sharpness(self, face_image):
        """×‘×“×™×§×ª ×—×“×•×ª ×”×¤× ×™× ×‘×ª××•× ×”"""
        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()

        is_sharp = sharpness >= self.MIN_SHARPNESS
        return is_sharp, sharpness

    def check_face_noise(self, face_image):
        """×‘×“×™×§×ª ×¨××ª ×”×¨×¢×© ×‘×ª××•× ×ª ×”×¤× ×™×"""
        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        noise = cv2.meanStdDev(gray)[1][0][0]

        is_clean = noise <= self.MAX_NOISE_THRESHOLD
        return is_clean, noise

    def check_face_contrast(self, face_image):
        """×‘×“×™×§×ª × ×™×’×•×“×™×•×ª ×‘×ª××•× ×ª ×”×¤× ×™×"""
        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        min_val, max_val = np.min(gray), np.max(gray)
        contrast = (max_val - min_val) / (max_val + min_val + 1e-6)  # ×× ×™×¢×ª ×—×œ×•×§×” ×‘××¤×¡

        has_good_contrast = contrast >= self.MIN_CONTRAST
        return has_good_contrast, contrast

    def extract_faces_from_directory(self, directory_path):
        """
        ×—×™×ª×•×š ×•×©××™×¨×ª ×”×¤× ×™× ××›×œ ×”×ª××•× ×•×ª ×‘×ª×™×§×™×™×”

        Args:
            directory_path (str): ×”× ×ª×™×‘ ×œ×ª×™×§×™×™×” ×”××›×™×œ×” ×ª××•× ×•×ª ×œ×¢×™×‘×•×“

        Returns:
            int: ××¡×¤×¨ ×”×¤× ×™× ×©×—×•×œ×¦×• ×‘×”×¦×œ×—×”
        """
        extracted_faces_count = 0

        try:
            if not os.path.exists(directory_path):
                print_status(f"×”×ª×™×§×™×™×” {directory_path} ×œ× × ××¦××”", emoji="âŒ")
                return extracted_faces_count

            image_extensions = ['.jpg', '.jpeg', '.png']
            image_files = []
            for ext in image_extensions:
                image_files.extend(glob.glob(os.path.join(directory_path, f'*{ext}')))

            if not image_files:
                print_status("×œ× × ××¦××• ×§×‘×¦×™ ×ª××•× ×” ×‘×ª×™×§×™×™×”", emoji="âŒ")
                return extracted_faces_count

            print_status(f"ğŸ” ×”×ª×—×œ×ª ×¡×¨×™×§×ª ×ª×™×§×™×™×”: {os.path.basename(directory_path)}", emoji="ğŸ“")
            print_status(f"× ××¦××• {len(image_files)} ×ª××•× ×•×ª ×œ×¡×¨×™×§×”", level=1)

            # ××™×“×¢ ×œ×¡×™×›×•×
            total_faces_found = 0
            images_with_faces = []
            start_time = time.time()

            for img_path in image_files:
                try:
                    img = cv2.imread(img_path)
                    if img is None:
                        print_status(f"×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××ª ×”×ª××•× ×”: {os.path.basename(img_path)}", level=1, emoji="âŒ")
                        continue

                    image_area = img.shape[0] * img.shape[1]
                    results = self.yolo_model(img, verbose=False)[0]
                    image_name = os.path.basename(img_path)
                    num_faces = len(results.boxes)

                    # ×©×•××¨ ××™×“×¢ ×¢×œ ×”×ª××•× ×”
                    if num_faces > 0:
                        total_faces_found += num_faces
                        images_with_faces.append((image_name, num_faces))

                    faces_saved_from_image = 0

                    for i, box in enumerate(results.boxes):
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        face_area = (x2 - x1) * (y2 - y1)
                        face = img[y1:y2, x1:x2]

                        # ×‘×“×™×§×•×ª ××™×›×•×ª
                        size_ok, face_ratio = self.check_face_size(face_area, image_area)
                        sharpness_ok, sharpness = self.check_face_sharpness(face)
                        noise_ok, noise = self.check_face_noise(face)
                        contrast_ok, contrast = self.check_face_contrast(face)

                        # ×× ×›×œ ×”×‘×“×™×§×•×ª ×¢×‘×¨×• ×‘×”×¦×œ×—×”
                        if (size_ok and sharpness_ok and noise_ok and contrast_ok):
                            face_path = f"{self.output_dir}/face_{len(os.listdir(self.output_dir)) + 1}.jpg"
                            cv2.imwrite(face_path, face)
                            extracted_faces_count += 1
                            faces_saved_from_image += 1

                except Exception as img_error:
                    print_status(f"×©×’×™××” ×‘×¢×™×‘×•×“ ×ª××•× ×” {os.path.basename(img_path)}: {str(img_error)}", emoji="âš ï¸")
                    continue

            # ×¡×™×›×•× ×‘×˜×‘×œ×”
            elapsed_time = time.time() - start_time

            print_status(f"ğŸ“Š ×¡×™×›×•× ×¡×¨×™×§×ª ×ª×™×§×™×™×”: × ×¡×¨×§×• {len(image_files)} ×ª××•× ×•×ª, × ××¦××• {total_faces_found} ×¤× ×™×",
                         emoji="âœ…")

            if images_with_faces:
                table_data = [[i + 1, name, count] for i, (name, count) in enumerate(images_with_faces)]
                headers = ["××¡×¤×¨ ×¤× ×™×", "×©× ×§×•×‘×¥", "#"]
                print("\n" + tabulate(table_data, headers=headers, tablefmt="grid", stralign="center"))

            print_status(f"×¡×”\"×› ×—×•×œ×¦×• {extracted_faces_count} ×¤× ×™× ××™×›×•×ª×™×•×ª ×•× ×©××¨×• ×‘×××’×¨", level=1)
            print_status(f"â±ï¸ ×–××Ÿ ×¡×¨×™×§×” ×›×•×œ×œ: {elapsed_time:.2f} ×©× ×™×•×ª", emoji="âœ…")

            return extracted_faces_count

        except Exception as e:
            print_status(f"×©×’×™××” ×‘×¢×™×‘×•×“ ×”×ª×™×§×™×™×”: {str(e)}", emoji="âŒ")
            return extracted_faces_count

# ×××¤×©×¨ ×œ×”×¨×™×¥ ××ª ×”×§×•×‘×¥ ×’× ×›×ª×¡×¨×™×˜ ×¢×¦×××™
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='×—×™×œ×•×¥ ×¤× ×™× ××ª×™×§×™×™×ª ×ª××•× ×•×ª')
    parser.add_argument('--input_dir', type=str, required=True, help='×ª×™×§×™×™×ª ×”×§×œ×˜ ×”××›×™×œ×” ×ª××•× ×•×ª')
    parser.add_argument('--output_dir', type=str, default='./EnviroFaces', help='×ª×™×§×™×™×ª ×”×¤×œ×˜ ×œ×©××™×¨×ª ×”×¤× ×™×')
    parser.add_argument('--model_path', type=str, default='./face_yolov8n.pt', help='× ×ª×™×‘ ×œ××•×“×œ YOLO')

    args = parser.parse_args()

    extractor = FaceExtractor(model_path=args.model_path, output_dir=args.output_dir)
    num_faces = extractor.extract_faces_from_directory(args.input_dir)

    print(f"×¡×”\"×› ×—×•×œ×¦×• {num_faces} ×¤× ×™×")